{"cells":[{"cell_type":"markdown","id":"daeed615-b6cc-477e-bc23-652177ce7286","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","<h1>Model Evaluation and Refinement with Tidymodels</h1>\n","\n","Estimated Time Needed: **45 min**\n"]},{"cell_type":"markdown","id":"bc6f55b7-6666-44ca-8407-3c9bfcb7061e","metadata":{},"outputs":[],"source":["### Welcome!\n","\n","We have built models and made predictions of flight delays. Now we will determine how accurate these predictions are. \n","\n","You will learn techniques for evaluating the performance of your models. This inludes how to split your dataset into training and testing sets, build and train linear regression models with a training set, compute metrics to assess the performance of models, and tune hyperparameters. Moreover, you’ll also learn a technique for handling cases with small datasets.\n"]},{"cell_type":"markdown","id":"7f481fc9-31db-4486-94f5-9fb633578750","metadata":{},"outputs":[],"source":["## Table of Contents\n","\n","* [1. Model Evaluation](#evaluation)\n","* [2. Over-fitting, Under-fitting and Model Selection](#selection)   \n","* [3. Regularization: Ridge regression, Lasso regression and Elastic Net](#ridge)\n","* [4. Grid Search](#grid)\n","* [5. Compare Model Performance](#compare)\n"]},{"cell_type":"markdown","id":"ad2af265-bd6b-4382-a298-545575a50bde","metadata":{},"outputs":[],"source":["## Setup\n","<p>\n","Before you can start diving into model evaluation, you first need to load the libraries and data.\n"]},{"cell_type":"markdown","id":"440a67c8-723c-4829-896a-f9eca04417e7","metadata":{},"outputs":[],"source":["<h4>Load Libraries</h4> \n","\n","In the previous labs, you used base R's `lm()` to create linear regression models. In this lab, we will introduce another way to create models with **Tidymodels**. \n","\n","Tidymodels is a collection of packages that use tidyverse principles to easily do the entire modeling process from preprocessing initial data, to creating a model, to tunning hyperparameters. The tidymodels packages can be used to produce high quality statistical and machine learning models. Our Jupyter notebook platforms have a built-in Tidyverse, Tidymodels and rlang packages so we do not need to install these packages prior to loading library. However, if you decide to run this lab on your RStudio Desktop locally on your machine, you can remove the commented lines of code to install these packages before loading.\n"]},{"cell_type":"code","id":"34af8082-ab5a-4ac5-9e40-c72f992053f2","metadata":{},"outputs":[],"source":["# Install tidymodels if you haven't done so\n# install.packages(\"rlang\")\n# install.packages(\"tidymodels\")"]},{"cell_type":"markdown","id":"3fe7b6fe-cbc4-4025-aa1c-504ebaea2f6a","metadata":{},"outputs":[],"source":["After installing the packages, load them. As with the other labs, we will be using \"tidyverse\" as well.\n"]},{"cell_type":"code","id":"0ac1f933-de53-4d57-9e44-b2efcaca88ac","metadata":{},"outputs":[],"source":["# Library for modeling\nlibrary(tidymodels)\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Load Metrics\nlibrary(Metrics)"]},{"cell_type":"markdown","id":"b8c5e19c-843f-47fa-9ef3-0b0ea2fe4afd","metadata":{},"outputs":[],"source":["#### Load Data\n"]},{"cell_type":"markdown","id":"f61420ea-dfc6-4e8f-bc51-57439ad88772","metadata":{},"outputs":[],"source":["As a reminder, you can find the \"Airline Data Set\" from the following link: <a href=\"https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz\">https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz</a>. \n","We will be using the LAX to JFK sample data set throughout this course.\n","</p>\n"]},{"cell_type":"code","id":"3f6bb537-dee5-463e-8513-1b44b3577389","metadata":{},"outputs":[],"source":["# url where the data is located\nurl <- \"https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz\"\n\n# download the file\ndownload.file(url, destfile = \"lax_to_jfk.tar.gz\")\n\n# untar the file so we can get the csv only\n# if you run this on your local machine, then can remove tar = \"internal\" \nuntar(\"lax_to_jfk.tar.gz\", tar = \"internal\")\n\n# read_csv only \nsub_airline <- read_csv(\"lax_to_jfk/lax_to_jfk.csv\",\n                     col_types = cols('DivDistance' = col_number(), \n                                      'DivArrDelay' = col_number()))"]},{"cell_type":"markdown","id":"9324504f-480c-4cb0-8cc0-60a0f3f017a3","metadata":{},"outputs":[],"source":["<a class=\"anchor\" id=\"evaluation\"></a>\n","## 1. Model Evaluation\n","### 1.1 Training and Testing Data\n","\n","An important step in testing your model is to split your data into training and testing data. The training data will be used to train (fit) models, while the testing data will not be touched until we are evaluating the model.\n","\n","Using other packages or programming languages may require to separate out the reponse variable (`ArrDelayMinutes` in this case) into another dataframe, but here that is not necessary. The response and predictor variables can all stay in one dataframe.\n","\n","Before splitting the data we:\n","* Use the principles learned in module 2 and use `replace_na()` to replace the NAs in the variables we are using to predict. Here, we choose to replace the values with 0 because having NA in these variables mean that there was no delay. \n","* Use `select()` to only include the variables we will use to create a final model.\n"]},{"cell_type":"code","id":"b725e43c-8c66-4992-9682-a6fabafb9f84","metadata":{},"outputs":[],"source":["flight_delays <- sub_airline %>% \n    replace_na(list(CarrierDelay = 0,\n                    WeatherDelay = 0,\n                    NASDelay = 0,\n                    SecurityDelay = 0,\n                    LateAircraftDelay = 0)) %>%\n    select(c(ArrDelayMinutes, DepDelayMinutes, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, DayOfWeek, Month))"]},{"cell_type":"markdown","id":"8c21c5b5-a14c-4c78-8f04-5e4a42a42005","metadata":{},"outputs":[],"source":["Now, with the prepared dataset `flight_delays`, you can split the data. A random seed is set so that the way the data is split will be the same every time this code is run, this helps create reproducible results. \n"]},{"cell_type":"code","id":"de651095-8f35-4459-a129-62c1210e32c3","metadata":{},"outputs":[],"source":["set.seed(1234)\nflight_split <- initial_split(flight_delays)\ntrain_data <- training(flight_split)\ntest_data <- testing(flight_split)"]},{"cell_type":"markdown","id":"bc077d54-17b0-42b4-a916-28bc0279fefc","metadata":{},"outputs":[],"source":["In `initial_split()`, you can also set the `prop` parameter to set the proportion of the data to use for training. If it is unspecified like here in the example, then by default it is set to 0.75. This means that the proportion of data that is split into the training data is 75% (so the testing data is 25%).\n"]},{"cell_type":"markdown","id":"3c0c6aa1-3ff7-4a70-89d6-2fddd076d7b9","metadata":{},"outputs":[],"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #1):</h1>\n","\n","<b>Use the function \"initial_split\" to split up the data set such that 80% of the data samples will be utilized for training. The output of the function should be the following:  \"flight_split2\", \"train_data2\" , \"test_data2\".</b>\n","</div>\n"]},{"cell_type":"code","id":"bc79c211-61dd-4167-9968-51e8a7a415c6","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"42e8ca0f-5954-4938-a147-6967ff4bff63","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","flight_split2 <- initial_split(flight_delays, prop = 4/5)  # prop = 0.8 works as well\n","train_data2 <- training(flight_split2)\n","test_data2 <- testing(flight_split2)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"e2b9048d-69df-414c-b78c-107c7e0a3a8c","metadata":{},"outputs":[],"source":["### 1.2 Training a Model\n","\n","After splitting the dataset, the next step is to create a Linear Regression object by using `linear_reg()` to specify linear regression and `set_engine()` to specify which package is used to create the model.\n"]},{"cell_type":"code","id":"2cf1f813-e15b-4d51-a669-520049eb1fb6","metadata":{},"outputs":[],"source":["# Pick linear regression\nlm_spec <- linear_reg() %>%\n  # Set engine'\n  set_engine(engine = \"lm\")\n\n# Print the linear function\nlm_spec"]},{"cell_type":"markdown","id":"78a5f720-7fbc-4dc3-b252-6034dfb51af4","metadata":{},"outputs":[],"source":["In this example, we will use Arrival Delay Minutes (\"ArrDelayMinutes\") as the response variable and Departure Delay Minutes (\"DepDelayMinutes\") as the predictor variable to fit (train) a model. We will use `train_data` because we are training the model. The `test_data` will be used later. \n","\n","Use `fit()` to fit the model we just specified in `lm_spec`. The output is the fitted (trained) model.\n"]},{"cell_type":"code","id":"dd54ec45-ff1f-4c25-8883-3001c3550af0","metadata":{},"outputs":[],"source":["train_fit <- lm_spec %>% \n    fit(ArrDelayMinutes ~ DepDelayMinutes, data = train_data)\n\ntrain_fit "]},{"cell_type":"markdown","id":"5ab3a772-6239-4854-b7b5-103470aa00e0","metadata":{},"outputs":[],"source":["To look at some of the predictions of the fitted model, use `predict()`, which will output one column with predictions (`.pred`). Here, since `new_data = train_data`, you are looking at how well the model is predicting the original training data.\n"]},{"cell_type":"code","id":"15e37230-4a34-419f-b3d8-bb44ea871447","metadata":{},"outputs":[],"source":["train_results <- train_fit %>%\n  # Make the predictions and save the predicted values\n  predict(new_data = train_data) %>%\n  # Create a new column to save the true values\n  mutate(truth = train_data$ArrDelayMinutes)\n\nhead(train_results)"]},{"cell_type":"markdown","id":"c473c43c-bc5b-47ba-9950-125818ed10fa","metadata":{},"outputs":[],"source":["Additionally, you can use the same fitted model to predict on test data and save to a dataset called `test_results`. There are two columns in the dataset, including both predicted values and true values. \n","\n","Now it is time to evaluate the models to estimate how well the models perform on new data, the test data. This example uses the same model in `train_fit` to make the predictions. Again, from `predict()`, the output is stored in a data frame with only one column, called `.pred`. You can then add a new column to this data frame using the `mutate()` function. This new column is named `truth` and contains values of \"ArrDelayMinutes\" from the `test_data`. In the end, you will have a dataframe with the predictions and the true values.\n"]},{"cell_type":"code","id":"45a88ee9-581e-4329-8e35-405dd2cdd63b","metadata":{},"outputs":[],"source":["test_results <- train_fit %>%\n  # Make the predictions and save the predicted values\n  predict(new_data = test_data) %>%\n  # Create a new column to save the true values\n  mutate(truth = test_data$ArrDelayMinutes)\n\nhead(test_results)"]},{"cell_type":"markdown","id":"003302bd-03c2-4e20-8653-14edd5bacbe7","metadata":{},"outputs":[],"source":["### 1.3 Evaluating the Model\n","\n","Next, let's evaluate the model. Using metrics learned in previous lessons like RMSE or R$^2$ are good ways to evaluate *regression* models. \n","\n","In previous lessons you learned how to claculate RMSE with combinations of functions like `mean()` and `sqrt()`, which is a good exercise. However in practice, this may not be ideal. So more conveniently with \"tidymodels\", there are already functions like `rmse()` as well as many other metric functions (see https://yardstick.tidymodels.org/reference/index.html).\n"]},{"cell_type":"code","id":"ef9ca3c3-e549-4c10-b854-bf8baec51331","metadata":{},"outputs":[],"source":["rmse_train1 <- rmse(train_results, truth = truth,\n     estimate = .pred)\nrmse_train1"]},{"cell_type":"code","id":"72866c45-53b1-41ee-ba72-c5e704815abb","metadata":{},"outputs":[],"source":["rmse_test1 <- rmse(test_results, truth = truth,\n     estimate = .pred)\nrmse_test1"]},{"cell_type":"markdown","id":"d1790381-2033-4f50-8e35-2535863cdbdb","metadata":{},"outputs":[],"source":["Using `rsq()`, let's lalculate the R-squared on the training and test data:\n"]},{"cell_type":"code","id":"5c45ef5f-2fd9-44c6-9654-6418b134663d","metadata":{},"outputs":[],"source":["rsq_train1 <- rsq(train_results, truth = truth,\n    estimate = .pred)\nrsq_train1 "]},{"cell_type":"code","id":"c6d17c4e-6a7b-465d-ad7a-4913140f1943","metadata":{},"outputs":[],"source":["rsq_test1 <- rsq(test_results, truth = truth,\n    estimate = .pred)\nrsq_test1 "]},{"cell_type":"markdown","id":"270d3152-e384-4b5b-b0a5-de2d20591a93","metadata":{},"outputs":[],"source":["You can also make a plot to visualize how well you predicted the Arrival Delay Minutes. \n","\n","This example plots the actual values (the true values of ArrDelayMinutes) versus the model predictions for both the testing and training datasets. It also plots the line y = x through the origin. This line is a visual representation of the perfect model where all predicted values are equal to the true values in the test set. The farther the points are from this line, the worse the model fit.\n","\n","Let's break down the code below:\n","1. `mutate` - add column called `train` to test_results and set the values all to \"testing\"\n","2. `bind_rows` - do the same to the train_results and bind these rows the test_results\n","3. `ggplot` - plot the truth vs prediction values\n","4. `geom_abline` - add the y=x line\n","5. `geom_point` - add the truth vs prediction points to the plot\n","6. `facet_wrap` - since `train` contains two values \"testing\" and \"training\", this splits the data into two graphs\n","7. `labs` - add labels\n"]},{"cell_type":"code","id":"7dbe7903-3689-4e91-8536-d29333c26bb9","metadata":{},"outputs":[],"source":["test_results %>%\n  mutate(train = \"testing\") %>%\n  bind_rows(train_results %>% mutate(train = \"training\")) %>%\n  ggplot(aes(truth, .pred)) +\n  geom_abline(lty = 2, color = \"orange\", \n              size = 1.5) +\n  geom_point(color = '#006EA1', \n             alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(x = \"Truth\", \n       y = \"Predicted Arrival Delays (min)\")"]},{"cell_type":"markdown","id":"01f6d94c-027e-4164-a717-5544de7f50fd","metadata":{},"outputs":[],"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #2): </h1>\n","<p> \n","Using \"ArrDelayMinutes\" as the response variable and \"DepDelayMinutes\" as the predictor variable, find the R^2  on the test data using 80% of the data for training data.\n","</p>\n","<p>Hint: use train_data2 from question 1. </p>\n","</div>\n"]},{"cell_type":"code","id":"977e0ba2-7ca8-4c42-b5be-957179b7a30f","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"a2ad0a54-ed85-4cae-9568-d6cd7aa3c8b0","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","# Write your code below and press Shift+Enter to execute \n","train_fit2 <- lm_spec %>% \n","    fit(ArrDelayMinutes ~ DepDelayMinutes, \n","    data = train_data2)\n","\n","train_results2 <- train_fit2 %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = train_data2) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = train_data2$ArrDelayMinutes)\n","\n","test_results2 <- train_fit2 %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = test_data2) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = test_data2$ArrDelayMinutes)\n","\n","\n","rmse_train2 <- sqrt(mean((train_results2$truth - train_results2$.pred)^2))\n","rsq_train2 <- rsq(train_results2, truth = truth, estimate = .pred)\n","rmse_train2\n","rsq_train2\n","\n","\n","rmse_test2 <- sqrt(mean((test_results2$truth - test_results2$.pred)^2))\n","rsq_test2 <- rsq(test_results2, truth = truth, estimate = .pred)\n","rmse_test2\n","rsq_test2\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"f23be2d7-6398-4ab0-9223-bb31bda0635f","metadata":{},"outputs":[],"source":["### 1.4 Multi Linear Regression\n"]},{"cell_type":"markdown","id":"16ae5930-9bf7-4730-821d-60e1762dde9b","metadata":{},"outputs":[],"source":["Let's predict arrival arrival delay minutes using <b>two or more</b> predictor (independent) variables 'Departure Delay Minutes' and 'Late Arrival Delay'. Most of the real-world regression models involve multiple predictors. We will use Multi-Linear Regression and predict the model performance.\n"]},{"cell_type":"code","id":"fd704277-42d9-48ab-9c70-4a3761bf1056","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"70c11bc7-2878-44f2-8f1b-2917a75493bd","metadata":{},"outputs":[],"source":["Sometimes you may not have a large enough testing data; as a result, you may want to perform cross validation. Let's  go over several methods that you can use for  cross validation. \n"]},{"cell_type":"markdown","id":"e476e7fe-5ffb-438d-b049-288df5b2ab72","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","# Write your code below and press Shift+Enter to execute \n","train_fit3 <- lm_spec %>% \n","    fit(ArrDelayMinutes ~  DepDelayMinutes + LateAircraftDelay + WeatherDelay, \n","    data = train_data)\n","\n","train_results3 <- train_fit3 %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = train_data) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = train_data$ArrDelayMinutes)\n","\n","test_results3 <- train_fit3 %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = test_data) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = test_data$ArrDelayMinutes)\n","\n","rmse_train3 <- sqrt(mean((train_results3$truth - train_results3$.pred)^2))\n","rsq_train3 <- rsq(train_results3, truth = truth, estimate = .pred)\n","rmse_train3\n","rsq_train3\n","\n","rmse_test3 <- sqrt(mean((test_results3$truth - test_results3$.pred)^2))\n","rsq_test3 <- rsq(test_results3, truth = truth, estimate = .pred)\n","rmse_test3\n","rsq_test3\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"90a45bab-1cc6-4f58-ae36-63389e6da4b8","metadata":{},"outputs":[],"source":["### 1.4 Polynomial Regression\n"]},{"cell_type":"markdown","id":"787c95bb-a063-48bf-929a-313c67634455","metadata":{},"outputs":[],"source":["We will try to improve the model by using polynomial model. \n"]},{"cell_type":"code","id":"410a9751-2515-448f-b413-b6df654f71d9","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"a1dbddb8-cb6b-4717-9c03-3dcf9be48a5d","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","# Write your code below and press Shift+Enter to execute \n","train_fit4 <- lm_spec %>% \n","    fit(ArrDelayMinutes ~  poly(DepDelayMinutes, 3), \n","    data = train_data)\n","\n","train_results4 <- train_fit4 %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = train_data) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = train_data$ArrDelayMinutes)\n","\n","test_results4 <- train_fit4 %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = test_data) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = test_data$ArrDelayMinutes)\n","\n","rmse_train4 <- sqrt(mean((train_results4$truth - train_results4$.pred)^2))\n","rsq_train4 <- rsq(train_results4, truth = truth, estimate = .pred)\n","rmse_train4\n","rsq_train4\n","\n","rmse_test4 <- sqrt(mean((test_results4$truth - test_results4$.pred)^2))\n","rsq_test4 <- rsq(test_results4, truth = truth, estimate = .pred)\n","rmse_test4\n","rsq_test4\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"ac1f67ee-e479-4846-91cb-0e3847885887","metadata":{},"outputs":[],"source":["### 1.4 Cross validation\n"]},{"cell_type":"markdown","id":"e56ada2c-2e3e-4b0e-8d6c-8a57b910536a","metadata":{},"outputs":[],"source":["One of the most common “out-of-sample” evaluation techniques is **cross validation**. \n","\n","Cross validation is an effective use of data because each observation is used for both training and testing. In cross validation, \n","1. First, the dataset is split into k-equal groups; each group is referred to as a fold. \n","2. k - 1 of the folds are used to train a model, and the remaining fold is used to test with an evaluation metric. \n","3. This is **repeated** until each of the k groups is used as the test set.\n","4. After all folds are used, there are k evaluation metric results. They are **averaged** to get an estimate of out-of-sample error\n","\n","For example, in 4-fold cross validation you would use three folds for training and then use one fold for testing. The same model would be trained and then tested 4 times using an evaluation metric. The evaluation metric that you use depends on the model, we will use RMSE and R-squared in our code example.\n","\n","\n","#### Why is it worth the effort to perform cross validation?\n","\n","Using cross validation means that a model is trained and evaluated many (k) times, however it is still worth the computational cost because it is used to test the generalizability of the model. Generalizability is a measure of how useful the results of a study are for a broader group of people and situations. As you train a model on the training set, it tends to overfit most of the time. To avoid this situation, you can use regularization techniques. Cross validation provides a check on how the model is performing on a test data (new unseen data), and since you have limited training instances, you need to be careful while reducing the amount of training samples and reserving it for testing purposes.\n","\n","Moreover, cross validation still works well with a **small amount of data**. For example, assume that you only have 100 samples. If you do a train test with an 80 to 20 percent split, then you only have 20 samples in the test set, which is too small to generalize reliable results. With cross validation, you can have as many as k-folds, so you can build k different models. In this case, you can make predictions on all your data and then average out the model performance.\n","\n","#### Code Example\n","To perform cross validation, you can use `vfold_cv()`. Setting `v = 10` means that it will use 10 folds. The function `fit_resamples()` will keep refitting the model specified on the samples specified by the cross validation object.\n"]},{"cell_type":"code","id":"9d1b108e-b763-48cc-ae83-cadd49a7559e","metadata":{},"outputs":[],"source":["set.seed(1234)\ncv_folds <- vfold_cv(train_data, v = 10)\nresults <- fit_resamples(lm_spec, \n                         ArrDelayMinutes ~ DepDelayMinutes,\n                         resamples = cv_folds)"]},{"cell_type":"markdown","id":"30b42fc2-4071-4972-80e6-39d54412545d","metadata":{},"outputs":[],"source":[" We can calculate the **average** RMSE and R-squared of our estimate:\n"]},{"cell_type":"code","id":"4dc74072-47bc-4491-a9ed-799b2e4f7cda","metadata":{},"outputs":[],"source":["results_cv <- results %>% collect_metrics()\nresults_cv\nrmse_train5 <- results_cv[1,3]\nrsq_train5 <- results_cv[2,3]\nrmse_train5\nrsq_train5"]},{"cell_type":"markdown","id":"8519ff1d-16e0-44bc-8ff3-614e69da40fd","metadata":{},"outputs":[],"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #3): </h1>\n","<b> \n","Calculate the average RMSE and R-squared using three folds utilizing DepDelayMinutes as a feature : \n","</b>\n","</div>\n"]},{"cell_type":"code","id":"1c0ddb98-c943-45f5-ab5c-281af75bc320","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"033ba12d-9540-4bf5-a4f1-6a9eaab74c8f","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","# Write your code below and press Shift+Enter to execute \n","cv_folds_3 <- vfold_cv(train_data, v = 3)\n","results <- fit_resamples(\n","    lm_spec, \n","    ArrDelayMinutes ~ DepDelayMinutes, \n","    resamples = cv_folds_3)\n","results_cv <- results %>% collect_metrics()\n","results_cv\n","rmse_train6 <- results_cv[1,3]\n","rsq_train6 <- results_cv[2,3]\n","rmse_train6\n","rsq_train6\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"382031c8-a2d7-4ea1-b092-5cbbfbe8d572","metadata":{},"outputs":[],"source":["<a class=\"anchor\" id=\"selection\"></a>\n","## 2. Overfitting, Underfitting and Model Selection\n","\n","A part of the training dataset is typically set aside as the “test set” to measure the performance of the model and check for \"underfitting\" or \"overfitting\" of the models. Key is to find the right balance between overfitting, which gives too much predictive power to specific quirks in your data, and underfitting, which ignores important general features in your data.\n","\n","**What is Underfitting?**\n","\n","A model is termed Underfitting when a data model is too simplistic and fails to capture the underlying patterns or the relationship between the input and output variables accurately. It cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. High bias and low variance are good indicators of underfitting. An underfitted model performs poorly both on the training data and on unseen data because it lacks the complexity to represent the true relationships in the data, in other words, a model that is underfit will have high training and high testing errors.\n","\n","#### How to prevent underfitting?\n","\n","- Increase the model complexity\n","For example, instead of using a linear function with a polynomial with degree 1, you can use a polynomial with a higher degree. \n","\n","- Add more features to the training data \n","Your model may be underfitting because the training data is too simple.  It may lack the features that will make the model detect the relevant patterns to make accurate predictions. If there are not enough predictive features present, then more features or features with greater importance, should be introduced. \n","\n","- Try different models\n"," You can also try different models, such as regression trees or random forest.\n","\n","#### Code Example\n","Let's go over an example of underfitting using a simple dataset included with R called \"mtcars\". \n","We will predict the distance (`dist`) it takes for cars to stop using the car's speed (`speed`) using ggplot() function as shown in the code below. \n","\n","In this example model, the model is defined a line set to the mean of the car's stopping distance. Based on the plot, this model is underfitting because of the speeds less than 10 and greater than 20 are very far from the prediction line.\n","\n","For more details, please visit https://www.ibm.com/topics/underfitting webpage.\n"]},{"cell_type":"code","id":"97d148bc-3dbb-422d-8884-4c727494d1b2","metadata":{},"outputs":[],"source":["ggplot(cars, aes(x = speed, y = dist)) + \n    geom_point() + \n    geom_hline(yintercept = mean(cars$dist), \n               col = \"red\") "]},{"cell_type":"markdown","id":"edaaca9f-c3a5-4e5b-9043-7f7b1738a7da","metadata":{},"outputs":[],"source":["**What is Overfitting?**\n","\n","In machine learning, overfitting occurs when an algorithm fits too closely or even exactly to its training data, resulting in a model that can’t make accurate predictions or conclusions from unseen data other than the training data. Low error rates and a high variance are good indicators of overfitting.  If the training data has a low error rate and the test data has a high error rate, it signals overfitting.\n","\n","Identifying overfitting can be more difficult than underfitting because unlike underfitting, the training data performs at high accuracy in an overfitted model. To assess the accuracy of an algorithm, a technique called k-fold cross-validation is typically used. \n","\n","These differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.\n","\n","#### How to prevent overfitting?\n","\n","- Asses model performance for overfitting using Cross-validation\n","\n","As discussed in section  [1.4 Cross validation](#cv), first evaluate model performance using cross-validation technique. The model may exhibit exceptional performance on some folds but poorly on others. This pattern suggests that the model is overfitted to the training data and lacks generalization capability, yielding a highly complex model. \n","\n","- Reduce model complexity\n","For example, if the model is a high-degree polynomial, you can decrease the degree.  \n","\n","- Training with more data\n","Training on more samples from dataset may increase accuracy of the model, however it is computationally intensive.\n"," \n","- Feature selection \n","Feature selection is the process of identifying the most important factors within the training data and then eliminating the irrelevant or redundant ones. This helps to simplify your model to establish the dominant trend in the data. \n","\n","- Regularization\n","\n","If we're uncertain about which inputs to eliminate during the feature selection process, regularization methods can be especially beneficial. These methods help by automatically determining which features to retain or discard within the model, even when we're unsure about the specific inputs to remove. The regularization is an important technique to prevent overfitting. It is a technique you can use to reduce the complexity of the model by applying a “penalty” to the input parameters with the larger coefficients, which subsequently limits the amount of variance in the model.  You will learn more about this in the next section.\n","\n","For more details, please visit https://www.ibm.com/topics/overfitting webpage.\n","\n","#### Code Example\n","Going back to the example with the \"mtcars\" dataset, you can reduce the complexity of the model. In the previous underfitting example, a polynomial model of 8 degrees was used. Instead, you can use a polynomial of degree 1 or a simple linear regression model.\n","\n","Here you set the formula to y over x and plot the graph. In this example, we demonstrated how you can prevent overfitting and underfitting models by changing the model complexity.\n"]},{"cell_type":"code","id":"0c588a09-c4a3-4f33-a897-07d10ddc0f2e","metadata":{},"outputs":[],"source":["ggplot(cars, aes(x = speed, y = dist)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", \n                formula = y ~ poly(x, 8), \n                col = \"red\", se = FALSE) "]},{"cell_type":"markdown","id":"a1eacd41-01be-407a-9979-81c3bb48f215","metadata":{},"outputs":[],"source":["The model is fitting to the points in the top right. If this model received new speeds, it may not be able to predict accurate distances.\n"]},{"cell_type":"markdown","id":"88ea9851-18b7-4b9e-a9ea-e533f7c60df0","metadata":{},"outputs":[],"source":["Going back to the example with the \"cars\" dataset, you can reduce the complexity of the model. In the previous overfitting example, a polynomial model of 8 degrees was used. Instead, you can use a polynomial of degree 1 or a simple linear regression model. In R, you can set the formula to y over x. In this example, we demonstrated how you can prevent overfitting and underfitting models by changing the model complexity.\n"]},{"cell_type":"code","id":"97faf576-a0a5-4215-b4a2-b2308df00e77","metadata":{},"outputs":[],"source":["ggplot(cars, aes(x = speed, y = dist)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", \n                formula = y ~ x, \n                col = \"red\", \n                se = FALSE) "]},{"cell_type":"markdown","id":"deb6e35f-07c6-41e6-9a8a-12fa5d9fda7d","metadata":{},"outputs":[],"source":["<a class=\"anchor\" id=\"ridge\"></a>\n","\n","## 3. Regularization\n","\n","Regularization is a way to handle the problem of overfitting. It is a technique you can use to reduce the complexity of the model by adding a penalty on the different parameters of the model. After it is applied, the model will be less likely to fit the noise of the training data and will improve the generalization abilities of the model. So, regularization is a way of *avoiding overfitting* by restricting the magnitude of model coefficients.\n","\n","There are a few methods of regularizing linear models including \n","\n","* **Lasso (L1) regularization**: \n","\n","Lasso (least absolute shrinkage and selection operator) is primarily used for variable selection, that is, reducing the number of variables/features used in a model by shrinking the coefficients to zero. You will use this if you have many variables and think just a select few will be useful in a final model. The downside of Lasso is that its variable selection is unstable, as in, for correlated variables it will arbitrarily select one. Additionally, if the number of data point (n) is less than the number of features (p), then Lasso can select at most n of the features \n","\n","* **Ridge (L2) regularization**: \n","\n","If you don’t want to reduce the number of variables, you can use this. Ridge also works well when there is multi-collinearity in the features because it reduces the variance while increasing bias. The downside of ridge regression is that the bias in the model may be high. \n","\n","* **Elastic net (mix of L1 and L2) regularization**: \n","\n","Elastic net combines the benefits of Lasso and Ridge. It solves some of the issues that Lasso has when doing variable selection because it works well when the variables are highly correlated, and it can work when the number of variables is greater than the number of samples. The downside of the Elastic net regression is that it be computationally more expensive than Lasso or Ridge because it computes both L1 and L2 penalties. \n","\n","Among these widely used regularization techniques are, ridge and lasso regression.   \n","\n","For ridge and lasso regression, **lambda** is a **hyperparameter** that you choose.  Unlike in linear regression where the model learns the coefficients directly from the data, in ridge and lasso regression, the model does not learn the value of the lambda. Instead, lambda is set by the user beforehand. You  determine the best value of the hyperparameters by choosing the model with the best RMSE. \n","\n","Having a method like [**grid search**](#grid) can help make you choose this value of lambda easily. The goal of grid search is to find the values of the hyperparameters that results in the best model (the smallest error). This is known as tuning hyperparameters.   For example, if you are training a lasso or ridge regression model, the parameter could be lambda and the error metric could be RMSE. \n","\n","For more details on Regularization, please visit https://www.ibm.com/topics/regularization webpage.\n"]},{"cell_type":"markdown","id":"d765a9fe-7ba0-4524-ba5b-32c1a4eb86bf","metadata":{},"outputs":[],"source":["### Comparing Regularization Types\n","\n","Now that you know more about regularization, it is also good to understand when you would use a techinque over the other. \n","\n","* **Lasso (L1)**: \n","    * Pros: Lasso is primarily used for variable selection, that is, reducing the number of variables/features used in a model by shrinking the coefficients to zero. You would use this if you have many variables and think just a select few would will be useful in a final model. \n","    * Cons: The downside of Lasso is that its variable selection is unstable, as in, for correlated variables it will arbitrarily select one. Additionally, if the number of data point (n) is less than the number of features (p), then Lasso can select at most n of the features.\n","\n","* **Ridge (L2)**: \n","    * Pros: If you don’t want to reduce the number of variables, you can use this. Ridge also works well when there is multicollinearity in the features because it reduces the variance while increasing bias.\n","    * Cons: Will not reduce the number of variables if that is your goal. Also, the bias in the model may be high.\n","\n","* **Elastic net (L1/L2)**: \n","    * Pros: Elastic net combines the benefits of Lasso and Ridge. It solves some of the issues that Lasso has when doing variable selection because it works well when the variables are highly correlated and it can work when the number of variables is greater than the number of samples.\n","    * Cons: May be computationally more expensive than Lasso or Ridge because it computes both L1 and L2 penalties.\n"]},{"cell_type":"markdown","id":"0e034b38-30ef-4185-b336-e81dc2866873","metadata":{},"outputs":[],"source":["## Code Example\n","### Ridge (L2) regularization\n","The example below illustrates the application of Ridge Regularization for model refinement.\n","\n","\n","### Step 1: Create a Recipe: \n","\n","A recipe specifies the data preprocessing steps and includes the model formula. You could preprocess the data more in this step, but the data here is already pre-processed. Hence, we only need to specify the data using the `recipe( )` function from the `tidymodels` library. The dot `.` in the formula is a special character that tells R to use all the variables in train_data.\n"]},{"cell_type":"code","id":"0891551d-7cd8-4b3e-8e2e-4456a9cb2bb1","metadata":{},"outputs":[],"source":["flight_recipe <-\n  recipe(ArrDelayMinutes ~ ., data = train_data)"]},{"cell_type":"markdown","id":"70ffd1a1-867b-4a6c-882c-906d667995db","metadata":{},"outputs":[],"source":["### Step 2: Specify the Model: Ridge (L2) regularization\n"]},{"cell_type":"markdown","id":"bbd60a51-66ab-4259-ae57-7a6cea2c5363","metadata":{},"outputs":[],"source":["Next, use the `linear_reg()` function from the tidymodels library to specify the model.  In ridge regression, we set mixture = 0 to indicate that only the L2 penalty (ridge penalty) is used. For lasso regression, specify Mixture = 1. \n","\n","* “**Penalty**” is the value of lambda.  \n","\n","* ”**Mixture**” is the proportion of L1 penalty. For ridge regression, specify Mixture = 0. \n","  \n","* By executing `set_engine(\"glmnet\")` you're specifying that you want to use glmnet package in R, which provides functions for fitting generalized linear models regularized with L1 or L2 penalties, \n"]},{"cell_type":"code","id":"86ece2c5-0966-44a2-965b-386abf23d868","metadata":{},"outputs":[],"source":["ridge_spec <- linear_reg(penalty = 0.1, mixture = 0) %>%\n  set_engine(\"glmnet\")"]},{"cell_type":"markdown","id":"97f75b9a-20c6-4328-900d-e5234c1d8975","metadata":{},"outputs":[],"source":["### Step 3: Specify the work flow and fit the model\n","\n","Next, create a workflow object so you can more conveniently combine pre-processing, modeling, and post-processing requests.\n"]},{"cell_type":"code","id":"984cf375-35d1-4901-851d-117656f224f0","metadata":{},"outputs":[],"source":["ridge_wf <- workflow() %>%\n  add_recipe(flight_recipe)"]},{"cell_type":"markdown","id":"bf7a47e5-73cb-48c4-8459-a027c8ef6c96","metadata":{},"outputs":[],"source":["Finally, add the ridge model and fit the model.\n"]},{"cell_type":"code","id":"c0a5251e-01e7-4629-ad78-c46ea3fa3596","metadata":{},"outputs":[],"source":["ridge_fit <- ridge_wf %>%\n  add_model(ridge_spec) %>%\n  fit(data = train_data)"]},{"cell_type":"markdown","id":"714d07c5-0828-4bb1-ad02-df51526986e4","metadata":{},"outputs":[],"source":["### Step 4: View the Fit Model Results\n","\n","We use the `pull_workflow_fit()` function to view the results of the fitted ridge regression model. The estimate column contains the estimates of the coefficients learned by the model, and the Penalty column contains the value of lambda, which in this example is 0.1.  \n"]},{"cell_type":"code","id":"39485994-7a23-4f57-9232-ae1f64e1bdc1","metadata":{},"outputs":[],"source":["ridge_fit %>%\n  pull_workflow_fit() %>%\n  tidy()"]},{"cell_type":"markdown","id":"42c43858-4a68-43b2-83e6-a3ad0174e67d","metadata":{},"outputs":[],"source":["Observe output table. The estimate column contains the estimates of the coefficients learned by the model. Penalty contains the value of lambda, which in this example is 0.1.\n"]},{"cell_type":"markdown","id":"73d1282c-dba0-4cd7-8ef3-e573f76de3f6","metadata":{},"outputs":[],"source":["### Lasso (L1) regularization\n","\n","Similarly, here is the code for lasso regression.\n"]},{"cell_type":"raw","id":"164aaaf8-091d-42a1-8e6f-5b48a3762c9c","metadata":{},"outputs":[],"source":["lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%\n  set_engine(\"glmnet\")\n\nlasso_wf <- workflow() %>%\n  add_recipe(flight_recipe)\n\nlasso_fit <- lasso_wf %>%\n  add_model(lasso_spec) %>%\n  fit(data = train_data)\n  \nlasso_fit %>%\n  pull_workflow_fit() %>%\n  tidy()   \n  "]},{"cell_type":"markdown","id":"1cf78286-1fb7-4bbd-9aa4-67ed88cd45e1","metadata":{},"outputs":[],"source":["### Elastic Net (L1 and L2) Regularization\n","\n","Moreover, here is the code for elastic net regularization. Like mentioned before, `mixture` is the proportion of L1 penalty used. Since elastic net uses a combination of L1 and L2 regularization, then when `mixture` is set to a value between 0 and 1 (not including 0 and 1) then it is considered elastic net regularization. In this example, it uses less L1 penalty than L2.\n"]},{"cell_type":"raw","id":"ca1c41f7-f8e1-48fd-963e-97b601460f2e","metadata":{},"outputs":[],"source":["elasticnet_spec <- linear_reg(penalty = 0.1, mixture = 0.3) %>%\n  set_engine(\"glmnet\")\n\nelasticnet_wf <- workflow() %>%\n  add_recipe(flight_recipe)\n  \nelasticnet_fit <- elasticnet_wf %>%\n  add_model(elasticnet_spec) %>%\n  fit(data = train_data)\n  \nelasticnet_fit %>%\n  pull_workflow_fit() %>%\n  tidy()"]},{"cell_type":"markdown","id":"6c9bd714-7b31-4015-b4ec-fd62a843c369","metadata":{},"outputs":[],"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #4): </h1>\n","\n","Perform elastic net regression with \"mixture = 0.5\" and \"penalty = 0.2\" using all features (variables) in the training data, and then output the result of the fitted regression model.\n","</div>\n"]},{"cell_type":"code","id":"23837040-3103-4a23-9dc8-7d87c2453b89","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"58809ee8-4d64-41b8-b434-0252c5acda62","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","flight_recipe <-\n","  recipe(ArrDelayMinutes ~ ., data = train_data)\n","\n","el_spec <- linear_reg(penalty = 0.5, mixture = 0.2) %>%\n","  set_engine(\"glmnet\")\n","\n","el_wf <- workflow() %>%\n","  add_recipe(flight_recipe)\n","\n","el_fit <- el_wf %>%\n","  add_model(el_spec) %>%\n","  fit(data = train_data)\n","\n","el_fit %>%\n","  pull_workflow_fit() %>%\n","  tidy()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"fbd1ac18-ca32-49da-b0c0-f1c98bbed655","metadata":{},"outputs":[],"source":["<a class=\"anchor\" id=\"grid\"></a>\n","## Part 4: Grid Search\n","\n","The goal of grid search is to find the values of the hyperparameters that results in the best model. This is known as tuning hyperparameters. Hyperparameters are parameters that are not derived from training the model. For example: 𝜆 (or lambda) in ridge/lasso is a hyperparameter.\n","\n","Grid search takes a list of values for each hyperparameter it is tuning and iterates through each combination. It then uses every combination of parameters to produce a model. For each model, a metric like RMSE is calculated. You then determine the best value of the hyperparameters by choosing the model with the best RMSE. In R, you can use functions in tidymodels to run grid search.\n"]},{"cell_type":"markdown","id":"243a803e-1f78-4bb2-b2ec-73cfe09f83e1","metadata":{},"outputs":[],"source":["First, define the lasso model. In this example, we will be tuning a lasso model so `mixture = 1`. We will tune lambda, which is `penalty` in the function.\n"]},{"cell_type":"code","id":"d4ae6fd6-d721-416d-87b0-8651d1d37e70","metadata":{},"outputs":[],"source":["tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n\nlasso_wf <- workflow() %>%\n  add_recipe(flight_recipe)"]},{"cell_type":"markdown","id":"8f6f2ee5-17b0-4675-b81b-eda7241e2d9a","metadata":{},"outputs":[],"source":["Next, define cross validation to resample the data:\n"]},{"cell_type":"code","id":"02dab7de-6130-4d10-8c6d-2c64dcca8da7","metadata":{},"outputs":[],"source":["flight_cvfolds <- vfold_cv(train_data)"]},{"cell_type":"markdown","id":"4c19af88-60fe-4301-87ef-08d3d9b4d461","metadata":{},"outputs":[],"source":["Now, you can set up the grid using `grid_regular()`. The `levels` are how many values to use and in `penalty()` you can specify the range of values to use. By default, the range values are inverse log transformed. This means that $-3$ is really $10^{-3}$ and $0.3$ is really $10^{0.3}$.\n"]},{"cell_type":"code","id":"9a696067-680b-419d-8908-1dc5d6cc3732","metadata":{},"outputs":[],"source":["lambda_grid <- grid_regular(levels = 50,\n  penalty(range = c(-3, 0.3)))"]},{"cell_type":"markdown","id":"5f05c03e-bbba-4a90-8dfb-68bc24dc88e3","metadata":{},"outputs":[],"source":["To tune the grid, use `tune_grid()` and include the lambda grid just specified.\n"]},{"cell_type":"code","id":"8a4817c9-aee2-4f41-a2fd-38475fec46c8","metadata":{},"outputs":[],"source":["lasso_grid <- tune_grid(\n    lasso_wf %>% add_model(tune_spec), \n    resamples = flight_cvfolds, \n    grid = lambda_grid)"]},{"cell_type":"markdown","id":"d7bda371-a16d-4cee-865e-4cdb2c2b686b","metadata":{},"outputs":[],"source":["Finally, to view best results for RMSE:\n"]},{"cell_type":"code","id":"aed02f21-4365-4ff5-a2e4-c3b7dbedcb2f","metadata":{},"outputs":[],"source":["results_grid <- show_best(lasso_grid, metric = \"rmse\")\nrmse_train7 <- results_grid[1,4]\nrmse_train7"]},{"cell_type":"markdown","id":"b587420f-fe6c-4c41-a972-b4baa27b75aa","metadata":{},"outputs":[],"source":["From the table and using RMSE as the metric, using lambda (penalty) equal to 1.25 gives the best result.\n","\n","Additionally, to visualize the RMSE results:\n"]},{"cell_type":"code","id":"4bfdd1a7-50a3-4a32-8917-ed479f8e318e","metadata":{},"outputs":[],"source":["lasso_grid %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") %>%\n  ggplot(aes(penalty, mean)) +\n  geom_line(size=1, color=\"red\") +\n  scale_x_log10() +\n  ggtitle(\"RMSE\")"]},{"cell_type":"markdown","id":"261aa316-ee1b-4372-8610-c1ecc437eaff","metadata":{},"outputs":[],"source":["The dip in the RMSE graph corresponds to the best value for lambda. So again, we see that using lambda (penalty) of about 1.25 gives the best result.\n"]},{"cell_type":"markdown","id":"81a6107d-cc2f-44a2-b308-8a35d687b0b3","metadata":{},"outputs":[],"source":["Finally, to view best results for RSQ:\n"]},{"cell_type":"code","id":"0f47ce4c-84bf-481c-bb42-e321233505ef","metadata":{},"outputs":[],"source":["results_grid <- show_best(lasso_grid, metric = \"rsq\")\nrsq_train7 <- results_grid[1,4]\nrsq_train7"]},{"cell_type":"markdown","id":"2fea40b6-9a74-487f-a08d-4fafad7a0d52","metadata":{},"outputs":[],"source":["From the table and using RSQ as the metric, using lambda (penalty) equal to 0.26 gives the best result.\n","\n","Additionally, to visualize the RSQ results:\n"]},{"cell_type":"code","id":"04a7316d-d831-43c4-a781-40be6e25e622","metadata":{},"outputs":[],"source":["lasso_grid %>%\n  collect_metrics() %>%\n  filter(.metric == \"rsq\") %>%\n  ggplot(aes(penalty, mean)) +\n  geom_line(size=1, color=\"red\") +\n  scale_x_log10() +\n  ggtitle(\"RSQ\")"]},{"cell_type":"markdown","id":"017af9b4-5e17-4ebb-87ef-f74c593c42f9","metadata":{},"outputs":[],"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #5): </h1>\n","Perform a grid search for the lambda (penalty) parameter on ridge regression, then find the best values of the parameter.\n","</div>\n"]},{"cell_type":"code","id":"9aa40284-3a09-4bf9-9133-0f0d3c152f09","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"e7bcb9d5-002d-4f76-9071-7d841870e901","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","tune_spec <- linear_reg( \n","    penalty = tune(), \n","    mixture = 0) %>% set_engine(\"glmnet\")\n","\n","ridge_wf <- workflow() %>%  add_recipe(flight_recipe)\n","\n","flight_cvfolds <- vfold_cv(train_data)\n","\n","lambda_grid <- grid_regular(levels = 50,  penalty(range = c(-3, 0.3)))\n","\n","\n","ridge_grid <- tune_grid(\n","    ridge_wf %>% \n","    add_model(tune_spec), \n","    resamples = flight_cvfolds, \n","    grid = lambda_grid)\n","\n","\n","show_best(ridge_grid, metric = \"rmse\")\n","\n","show_best(ridge_grid, metric = \"rsq\")\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"d35173ff-1c35-4068-96db-953b7f18418c","metadata":{},"outputs":[],"source":["<a class=\"anchor\" id=\"compare\"></a>\n","## 5. Comparing models performance\n","\n","A regression model is considered more accurate when it achieves a lower RMSE, as this indicates that its predictions closely match the actual observed values. Conversely, a higher R-squared value is preferred because it signifies that a larger proportion of the variance in the dependent variable is explained by the independent variables used in the model. Lets compare the values of RMSE and RSQ for all models for both test and train data.\n"]},{"cell_type":"code","id":"78bd64a9-81cc-4f06-a738-4838e8418ac9","metadata":{},"outputs":[],"source":["\nmodel <- c('lm1', 'lm2', 'mlr', 'poly', 'cv1', 'cv2', 'reg')\nrsq <- c(rsq_train1$.estimate, rsq_train2$.estimate, rsq_train3$.estimate, rsq_train4$.estimate,rsq_train5$mean, rsq_train6$mean, rsq_train7$mean )\nrmse <- c(rmse_train1, rmse_train2, rmse_train3, rmse_train4,rmse_train5$mean, rmse_train6$mean, rmse_train7$mean)\ntraindata_cm <- data.frame(model, rsq, rmse)\ntraindata_cm"]},{"cell_type":"markdown","id":"48b5c0dd-23b3-418b-a944-21142fb356de","metadata":{},"outputs":[],"source":["<h1>Thank you for completing this notebook!</h1>\n","\n","Checkout the documentation here: https://www.tidymodels.org/.\n"]},{"cell_type":"markdown","id":"3498a9f3-beac-4038-b6f0-b370e28979ce","metadata":{},"outputs":[],"source":["<h3>About the Authors:</h3>\n","\n","This notebook was written by <a href=\"https://www.linkedin.com/in/yiwen-li-47a019119/\" target=\"_blank\">Yiwen Li</a> and <a href=\"https://www.linkedin.com/in/gabrieladequeiroz/\" target=\"_blank\">Gabriela de Queiroz</a>.\n","\n","<p><a href=\"https://www.linkedin.com/in/yiwen-li-47a019119/\" target=\"_blank\">Yiwen Li</a> has approximately three year experiences in big tech industry. Currently, she is a developer advocate, a data scientist, a product manager at IBM, where she designs and develops data science solutions and Machine Learning models to solve real world problems. She has delivered talks this year in JupyterCon, PyCon, Pyjamas, CrowdCast.ai, Global AI on Tour 2020 and Belpy 2021 with hundreds of attendants per talk. \n","    \n","<a href=\"https://www.linkedin.com/in/gabrieladequeiroz/\" target=\"_blank\">Gabriela de Queiroz</a> is a Sr. Engineering & Data Science Manager at IBM where she manages and leads a team of developers working on Data & AI Open Source projects. She works to democratize AI by building tools and launching new open source projects.\n","She is the founder of AI Inclusive, a global organization that is helping increase the representation and participation of gender minorities in Artificial Intelligence. She is also the founder of R-Ladies, a worldwide organization for promoting diversity in the R community with more than 190 chapters in 50+ countries.\n","She has worked in several startups and where she built teams, developed statistical models, and employed a variety of techniques to derive insights and drive data-centric decisions\n"]},{"cell_type":"markdown","id":"2aa5cf91-a6cb-4269-8894-d6f7d4128f84","metadata":{},"outputs":[],"source":["<hr>\n","<p>Copyright &copy; 2021 IBM Corporation. All rights reserved.</p>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"prev_pub_hash":"7f5d234cb5b782b6577525a80a73da36f35f6b604746e73327cb014cd685e883"},"nbformat":4,"nbformat_minor":4}